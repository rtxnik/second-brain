# 21.03.02 Ceph

> Распределённое хранилище для Proxmox

## Общая информация

| Параметр | Значение          |
| -------- | ----------------- |
| Версия   | Ceph Squid (19.2) |
| Ноды     | 3                 |
| OSD      | 6 (2 на ноду)     |
| Сеть     | 10.0.11.0/24      |

---

## Архитектура

```
┌─────────────────────────────────────────────────┐
│                  Ceph Cluster                    │
├─────────────────┬─────────────────┬─────────────┤
│ pve-compute-01  │ pve-compute-02  │ pve-compute-03│
│                 │                 │              │
│ MON + MGR       │ MON + MGR       │ MON + MGR    │
│ OSD.0 (nvme0)   │ OSD.2 (nvme0)   │ OSD.4 (nvme0)│
│ OSD.1 (nvme1)   │ OSD.3 (nvme1)   │ OSD.5 (nvme1)│
└─────────────────┴─────────────────┴─────────────┘
```

---

## Диски

| Нода           | Диск         | OSD   | Размер |
| -------------- | ------------ | ----- | ------ |
| pve-compute-01 | /dev/nvme0n1 | OSD.0 | 2TB    |
| pve-compute-01 | /dev/nvme1n1 | OSD.1 | 2TB    |
| pve-compute-02 | /dev/nvme0n1 | OSD.2 | 2TB    |
| pve-compute-02 | /dev/nvme1n1 | OSD.3 | 2TB    |
| pve-compute-03 | /dev/nvme0n1 | OSD.4 | 2TB    |
| pve-compute-03 | /dev/nvme1n1 | OSD.5 | 2TB    |

**Raw:** 12TB  
**Usable:** ~3.8TB (replication factor 3)

---

## Pools

| Pool         | Size | PGs | Назначение         |
| ------------ | ---- | --- | ------------------ |
| proxmox-vms  | 3    | 128 | VM disks           |
| k8s-longhorn | 3    | 64  | Longhorn (будущее) |

---

## Сетевая конфигурация

```
Public Network:  10.0.11.0/24 (vmbr1)
Cluster Network: 10.0.11.0/24 (vmbr1)
```

> Используется одна сеть для обоих типов трафика (достаточно для homelab)

---

## Установка через Proxmox GUI

### 1. Установить Ceph

```
Datacenter → <node> → Ceph → Install
Version: Squid (19.2)
```

### 2. Создать MON и MGR на каждой ноде

```
Ceph → Monitor → Create
Ceph → Manager → Create
```

### 3. Создать OSDs

```
Ceph → OSD → Create Disk
Device: /dev/nvme0n1
Device: /dev/nvme1n1
```

### 4. Создать Pool

```
Ceph → Pools → Create
Name: proxmox-vms
Size: 3
Min Size: 2
PG Autoscale: On
```

### 5. Добавить Storage

```
Datacenter → Storage → Add → RBD
ID: ceph-pool
Pool: proxmox-vms
Monitor(s): 10.0.11.11 10.0.11.12 10.0.11.13
```

---

## Полезные команды

```bash
# Статус кластера
ceph status
ceph health detail

# OSDs
ceph osd tree
ceph osd status
ceph osd df

# Pools
ceph osd pool ls
ceph osd pool stats proxmox-vms
ceph df

# MON/MGR
ceph mon stat
ceph mgr services

# Performance
ceph osd perf
rados bench -p proxmox-vms 30 write --no-cleanup
rados bench -p proxmox-vms 30 seq
```

---

## Dashboard

```
URL: https://10.0.10.11:8443
Включить: ceph mgr module enable dashboard
```

---

## Troubleshooting

### Проверка здоровья

```bash
ceph health detail
ceph -w  # watch mode
```

### OSD не стартует

```bash
systemctl status ceph-osd@0
journalctl -u ceph-osd@0 -f
```

### Recovery

```bash
ceph pg stat
ceph osd pool set proxmox-vms recovery_max_active 3
```

---

## Статус

- [x] Ceph установлен
- [x] MON/MGR на всех нодах
- [x] 6 OSD созданы
- [x] Pool proxmox-vms создан
- [ ] Pool k8s-longhorn создан
- [ ] Benchmark проведён

---

Дата создания: 02-12-2025 18:22

#### Ссылки:

[[21.03.00 Proxmox]]
[[21.03.01 Кластер]]
